import streamlit as st
import pandas as pd
import os
import sys
import json
import time
from typing import Dict, List, Optional
from streamlit_option_menu import option_menu
import datetime
from celery.result import AsyncResult
import asyncio

# Proje kÃ¶k dizinini sisteme tanÄ±tarak diÄŸer modÃ¼lleri import et
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.celery_app import celery_app
from src.vector_db.embedding_service import AgentEmbeddingService
from src.rag.rag_pipeline import RAGPipeline
from src.evaluation.evaluator import AgentEvaluator, EvaluationMetrics
from src.tasks import batch_evaluate_task

# --- Sayfa YapÄ±landÄ±rmasÄ± ---
st.set_page_config(
    page_title="AI Agent DeÄŸerlendirme Paneli",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ğŸ¤– AI Agent DeÄŸerlendirme Paneli")
st.markdown("""
Bu panel, kaydedilmiÅŸ ajan konuÅŸmalarÄ±nÄ± analiz etmek ve ajanlarÄ±n performansÄ±nÄ± `Agent Goal'Ä±na Uygun DavranmÄ±ÅŸ mÄ±?` belgesindeki kriterlere gÃ¶re deÄŸerlendirmek iÃ§in tasarlanmÄ±ÅŸtÄ±r.
""")

# --- Servisleri BaÅŸlatma ---
@st.cache_resource
def initialize_services() -> Optional[AgentEvaluator]:
    """Gerekli servisleri baÅŸlatÄ±r ve Ã¶nbelleÄŸe alÄ±r."""
    try:
        # Servisler artÄ±k ayarlarÄ± config dosyasÄ±ndan otomatik olarak alÄ±yor.
        embedding_service = AgentEmbeddingService()
        evaluator = AgentEvaluator()
        return evaluator
    except Exception as e:
        st.error(f"Servisler baÅŸlatÄ±lÄ±rken bir hata oluÅŸtu: {e}")
        return None

evaluator = initialize_services()
FEEDBACK_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data", "feedback.csv")

# --- Geri Bildirim FonksiyonlarÄ± ---
def save_feedback(evaluation_data: dict, feedback: str):
    """KullanÄ±cÄ± geri bildirimini bir CSV dosyasÄ±na kaydeder."""
    try:
        feedback_df = pd.DataFrame([{"timestamp": datetime.datetime.now(), "feedback": feedback, **evaluation_data}])
        
        # Dosya zaten varsa, baÅŸlÄ±k olmadan ekle, yoksa dosyayÄ± oluÅŸtur
        header = not os.path.exists(FEEDBACK_FILE)
        feedback_df.to_csv(FEEDBACK_FILE, mode='a', header=header, index=False)
        st.toast(f"Geri bildiriminiz kaydedildi: {feedback}", icon="ğŸ‘" if feedback == "olumlu" else "ğŸ‘")
    except Exception as e:
        st.error(f"Geri bildirim kaydedilirken hata oluÅŸtu: {e}")

# --- Veri Ä°ÅŸleme FonksiyonlarÄ± ---
@st.cache_data
def process_chat_data(chats_df: pd.DataFrame, personas_df: pd.DataFrame, tasks_df: pd.DataFrame) -> pd.DataFrame:
    """Sohbet verilerini iÅŸleyip soru-cevap Ã§iftleri haline getirir."""
    # Ã‡akÄ±ÅŸmayÄ± Ã¶nlemek iÃ§in 'created_at' sÃ¼tunlarÄ±nÄ± birleÅŸtirmeden Ã¶nce yeniden adlandÄ±r
    personas_df.rename(columns={'created_at': 'persona_created_at'}, inplace=True)
    tasks_df.rename(columns={'created_at': 'task_created_at'}, inplace=True)
    
    personas_df = personas_df.sort_values('persona_created_at').drop_duplicates('agent_id', keep='last')
    tasks_df = tasks_df.sort_values('task_created_at').drop_duplicates('agent_id', keep='last')
    
    merged_df = pd.merge(chats_df, personas_df, on='agent_id', how='left')
    merged_df = pd.merge(merged_df, tasks_df, on='agent_id', how='left')
    
    user_chats = merged_df[merged_df['type'] == 'USER'].rename(columns={'content': 'user_query', 'created_at': 'user_time'})
    assistant_chats = merged_df[merged_df['type'] == 'ASSISTANT'].rename(columns={'content': 'agent_response', 'created_at': 'assistant_time'})
    
    user_chats['chat_rank'] = user_chats.groupby('chat_id').cumcount()
    assistant_chats['chat_rank'] = assistant_chats.groupby('chat_id').cumcount()
    
    qa_df = pd.merge(
        user_chats[user_chats['chat_rank'] == 0], 
        assistant_chats[assistant_chats['chat_rank'] > 0],
        on=['chat_id', 'agent_id'],
        suffixes=('_user', '_assistant')
    )
    
    # HATA DÃœZELTMESÄ°: BirleÅŸtirme sonrasÄ± _user ve _assistant olarak ayrÄ±lan sÃ¼tunlarÄ± dÃ¼zelt
    if 'persona_user' in qa_df.columns:
        qa_df['persona'] = qa_df['persona_user']
    if 'tasks_user' in qa_df.columns:
        qa_df['tasks'] = qa_df['tasks_user']

    qa_df = qa_df.sort_values('assistant_time').drop_duplicates('chat_id', keep='first')
    return qa_df.dropna(subset=['user_query', 'agent_response', 'persona', 'tasks'])

@st.cache_data
def load_default_data(data_path: str) -> pd.DataFrame:
    """VarsayÄ±lan veri dosyalarÄ±nÄ± yÃ¼kler ve iÅŸler."""
    try:
        chats_df = pd.read_csv(os.path.join(data_path, "ai_agent_chat_messages_june_18_25.csv"))
        personas_df = pd.read_csv(os.path.join(data_path, "ai_agent_persona_june_18_25.csv"))
        tasks_df = pd.read_csv(os.path.join(data_path, "ai_agent_tasks_june_18_25.csv"))
        return process_chat_data(chats_df, personas_df, tasks_df)
    except FileNotFoundError as e:
        st.error(f"Veri dosyasÄ± bulunamadÄ±: {e.filename}")
        return pd.DataFrame()

@st.cache_data
def load_and_merge_raw_data(data_path: str) -> pd.DataFrame:
    """VarsayÄ±lan veri dosyalarÄ±nÄ± yÃ¼kler ve birleÅŸtirir, ancak QA formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmez."""
    try:
        chats_df = pd.read_csv(os.path.join(data_path, "ai_agent_chat_messages_june_18_25.csv"))
        personas_df = pd.read_csv(os.path.join(data_path, "ai_agent_persona_june_18_25.csv"))
        tasks_df = pd.read_csv(os.path.join(data_path, "ai_agent_tasks_june_18_25.csv"))
        
        personas_df.rename(columns={'created_at': 'persona_created_at'}, inplace=True)
        tasks_df.rename(columns={'created_at': 'task_created_at'}, inplace=True)
    
        personas_df = personas_df.sort_values('persona_created_at').drop_duplicates('agent_id', keep='last')
        tasks_df = tasks_df.sort_values('task_created_at').drop_duplicates('agent_id', keep='last')
        
        merged_df = pd.merge(chats_df, personas_df, on='agent_id', how='left')
        merged_df = pd.merge(merged_df, tasks_df, on='agent_id', how='left')
        
        return merged_df.dropna(subset=['content', 'persona', 'tasks'])
    except FileNotFoundError as e:
        st.error(f"Veri dosyasÄ± bulunamadÄ±: {e.filename}")
        return pd.DataFrame()

# --- DeÄŸerlendirme Fonksiyonu ---
def run_evaluation(eval_data: pd.Series, _evaluator: AgentEvaluator) -> Optional[EvaluationMetrics]:
    """Tek bir konuÅŸma verisi iÃ§in deÄŸerlendirmeyi Ã§alÄ±ÅŸtÄ±rÄ±r."""
    try:
        user_query = str(eval_data['user_query'])
        agent_response = str(eval_data['agent_response'])
        agent_persona = str(eval_data['persona'])
        
        try:
            tasks_str = str(eval_data.get('tasks', '[]'))
            tasks = json.loads(tasks_str.replace("'", "\""))
            task_descriptions = [t.get('value', {}).get('about', '') for t in tasks if t.get('type') == 'talk-about']
            agent_goal = ". ".join(filter(None, task_descriptions)) or "KullanÄ±cÄ±ya yardÄ±mcÄ± olmak."
        except (json.JSONDecodeError, TypeError):
            agent_goal = "KullanÄ±cÄ±ya yardÄ±mcÄ± olmak."
        
        rag_context = f"Agent'Ä±n bilgi tabanÄ±ndan getirdiÄŸi varsayÄ±lan kanÄ±t: '{agent_response}'"

        return _evaluator.evaluate_conversation(
            user_query=user_query, agent_response=agent_response,
            agent_goal=agent_goal, rag_context=rag_context,
            agent_persona=agent_persona, tool_calls=None
        )
    except Exception as e:
        st.error(f"DeÄŸerlendirme hatasÄ±: {e}")
        return None

def display_evaluation_results(evaluation_result: Optional[EvaluationMetrics]):
    """DeÄŸerlendirme sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirir."""
    if not evaluation_result:
        st.error("DeÄŸerlendirme sonucu alÄ±namadÄ±.")
        return
        
    st.header("ğŸ“Š DeÄŸerlendirme SonuÃ§larÄ±")
    # DeÄŸerlendirmenin benzersiz bir anahtarÄ±nÄ± oluÅŸturmak iÃ§in session state kullan
    if 'current_eval_result' not in st.session_state or st.session_state.current_eval_result != evaluation_result:
        st.session_state.current_eval_result = evaluation_result
        st.session_state.feedback_given = False # Yeni sonuÃ§ iÃ§in geri bildirimi sÄ±fÄ±rla

    cols = st.columns(4)
    metrics_to_show = {
        "Goal Adherence": evaluation_result.goal_adherence,
        "Answer Relevance": evaluation_result.answer_relevance,
        "Groundedness": evaluation_result.groundedness,
        "Persona Compliance": getattr(evaluation_result, 'persona_compliance', None),
        "Style & Courtesy": getattr(evaluation_result, 'style_and_courtesy', None),
        "Conciseness": getattr(evaluation_result, 'conciseness', None),
        "Knowledge Boundary": evaluation_result.knowledge_boundary_violation,
        "Security/Policy": evaluation_result.security_policy_violation,
    }
    
    col_idx = 0
    for name, metric in metrics_to_show.items():
        if metric is None: continue
        with cols[col_idx % 4]:
            delta_color = "inverse" if name in ["Knowledge Boundary", "Security/Policy"] and metric.score > 0 else "normal"
            st.metric(label=name, value=f"{metric.score:.2f}", delta_color=delta_color)
            with st.expander("GerekÃ§eyi GÃ¶r"):
                st.write(metric.reasoning)
        col_idx += 1
    
    # --- Geri Bildirim BÃ¶lÃ¼mÃ¼ ---
    if not st.session_state.get('feedback_given', False):
        st.markdown("---")
        st.subheader("Bu DeÄŸerlendirme FaydalÄ± Oldu mu?")
        feedback_cols = st.columns(8)
        
        if feedback_cols[0].button("ğŸ‘ Olumlu", key=f"positive_feedback_{id(evaluation_result)}"):
            save_feedback(evaluation_result.model_dump(), "olumlu")
            st.session_state.feedback_given = True
            st.rerun()

        if feedback_cols[1].button("ğŸ‘ Olumsuz", key=f"negative_feedback_{id(evaluation_result)}"):
            save_feedback(evaluation_result.model_dump(), "olumsuz")
            st.session_state.feedback_given = True
            st.rerun()
    else:
        st.success("Bu deÄŸerlendirme iÃ§in geri bildiriminiz alÄ±nmÄ±ÅŸtÄ±r. TeÅŸekkÃ¼rler!")

async def run_session_evaluation(session_df: pd.DataFrame, _evaluator: AgentEvaluator) -> Optional[EvaluationMetrics]:
    """TÃ¼m bir oturumu deÄŸerlendirir."""
    try:
        full_conversation = []
        for _, row in session_df.sort_values('created_at').iterrows():
            full_conversation.append({"role": str(row['type']).lower(), "content": str(row['content'])})
        
        agent_persona = session_df['persona'].iloc[0]
        try:
            tasks_str = str(session_df['tasks'].iloc[0])
            tasks = json.loads(tasks_str.replace("'", "\""))
            task_descriptions = [t.get('value', {}).get('about', '') for t in tasks if t.get('type') == 'talk-about']
            agent_goal = ". ".join(filter(None, task_descriptions)) or "KullanÄ±cÄ±ya yardÄ±mcÄ± olmak."
        except (json.JSONDecodeError, TypeError):
            agent_goal = "KullanÄ±cÄ±ya yardÄ±mcÄ± olmak."

        return await _evaluator.evaluate_session(full_conversation, agent_goal, str(agent_persona))
    except Exception as e:
        st.error(f"Oturum deÄŸerlendirme hatasÄ±: {e}")
        return None

# --- NAVÄ°GASYON ---
with st.sidebar:
    # Projeyi taÅŸÄ±nabilir hale getirmek iÃ§in yerel ve gÃ¶receli bir yol kullanÄ±n.
    # 'use_column_width' genellikle daha iyi kalite iÃ§in geniÅŸliÄŸi optimize eder.
    st.image("src/assets/Jotform-New-Logo.png", use_container_width='auto')
    st.title("AI Agent DeÄŸerlendirme")
    
    page = option_menu(
        menu_title=None,
        options=["Sandbox", "Toplu DeÄŸerlendirme", "Oturum Analizi"],
        icons=["beaker", "collection-fill", "chat-left-text-fill"],
        menu_icon="cast",
        default_index=0,
    )

# --- Sayfa 1: Manuel DeÄŸerlendirme (Sandbox) ---
if page == "Sandbox":
    st.header("ğŸ§ª Manuel DeÄŸerlendirme (Sandbox)")
    st.markdown("""
    Bu bÃ¶lÃ¼mde, belirlediÄŸiniz bir senaryoya gÃ¶re ajanÄ±n potansiyel performansÄ±nÄ± deÄŸerlendirebilirsiniz. 
    AÅŸaÄŸÄ±daki alanlarÄ± doldurarak varsayÄ±msal bir durumu test edin veya hÄ±zlÄ± test butonuyla Ã¶nceden tanÄ±mlanmÄ±ÅŸ bir senaryoyu Ã§alÄ±ÅŸtÄ±rÄ±n.
    """)

    if st.button("âš¡ HÄ±zlÄ± Test Ã‡alÄ±ÅŸtÄ±r (GeÃ§ici)", use_container_width=True, type="secondary"):
        if evaluator:
            with st.spinner("HÄ±zlÄ± test senaryosu deÄŸerlendiriliyor..."):
                test_user_query = "Jotform'un Ã¼cretsiz planÄ±nda kaÃ§ form oluÅŸturabilirim ve bu formlara kaÃ§ yanÄ±t alabilirim?"
                test_agent_response = "Jotform'un Ã¼cretsiz planÄ±yla 5 adede kadar form oluÅŸturabilirsiniz. Bu formlar Ã¼zerinden aylÄ±k toplam 100 yanÄ±t alabilirsiniz. AyrÄ±ca 100 MB depolama alanÄ±nÄ±z olur."
                test_agent_persona = "YardÄ±msever, profesyonel ve Ã§Ã¶zÃ¼m odaklÄ± bir asistansÄ±nÄ±z. KullanÄ±cÄ±nÄ±n sorununu net bir ÅŸekilde anlayÄ±p etkili bir Ã§Ã¶zÃ¼m sunmaya odaklanmalÄ±sÄ±nÄ±z."
                test_agent_goal = "KullanÄ±cÄ±nÄ±n Jotform hakkÄ±ndaki sorularÄ±nÄ± yanÄ±tlamak ve onlara platformu en verimli ÅŸekilde nasÄ±l kullanacaklarÄ± konusunda rehberlik etmek."
                rag_context = f"Agent'Ä±n bilgi tabanÄ±ndan getirdiÄŸi varsayÄ±lan kanÄ±t: '{test_agent_response}'"
                
                result = asyncio.run(evaluator.evaluate_conversation(
                    user_query=test_user_query,
                    agent_response=test_agent_response,
                    agent_goal=test_agent_goal,
                    rag_context=rag_context,
                    agent_persona=test_agent_persona,
                    tool_calls=None
                ))
                st.session_state.eval_result = result
                
                with st.expander("Ã‡alÄ±ÅŸtÄ±rÄ±lan Test Verisi", expanded=True):
                    st.text_area("KullanÄ±cÄ± Sorusu", value=test_user_query, disabled=True, height=100)
                    st.text_area("Agent'Ä±n CevabÄ±", value=test_agent_response, disabled=True, height=150)
                    st.text_area("Agent'Ä±n PersonasÄ±", value=test_agent_persona, disabled=True, height=150)
                    st.text_area("Agent'Ä±n GÃ¶revi (Goal)", value=test_agent_goal, disabled=True, height=100)
        else:
            st.error("DeÄŸerlendirici servisi (Evaluator) baÅŸlatÄ±lamadÄ±.")

    with st.form(key="sandbox_form"):
        user_query = st.text_area("ğŸ‘¤ KullanÄ±cÄ± Sorusu", height=100, placeholder="Ã–r: Jotform'un sunduÄŸu farklÄ± abonelik planlarÄ± nelerdir?")
        agent_response = st.text_area("ğŸ¤– AjanÄ±n CevabÄ±", height=150, placeholder="Ã–r: Elbette, Jotform'da Ãœcretsiz, Bronz, GÃ¼mÃ¼ÅŸ ve AltÄ±n olmak Ã¼zere dÃ¶rt farklÄ± plan bulunmaktadÄ±r. Her planÄ±n form limiti, gÃ¶nderi sayÄ±sÄ± ve depolama alanÄ± gibi farklÄ± Ã¶zellikleri vardÄ±r. Ä°htiyaÃ§larÄ±nÄ±za en uygun olanÄ± seÃ§menize yardÄ±mcÄ± olabilirim.")
        agent_persona = st.text_area("ğŸ­ Ajan PersonasÄ±", height=150, value="YardÄ±msever, profesyonel ve Ã§Ã¶zÃ¼m odaklÄ± bir asistansÄ±nÄ±z. KullanÄ±cÄ±nÄ±n sorununu net bir ÅŸekilde anlayÄ±p etkili bir Ã§Ã¶zÃ¼m sunmaya odaklanmalÄ±sÄ±nÄ±z.")
        agent_goal = st.text_area("ğŸ¯ AjanÄ±n GÃ¶revi (Goal)", height=100, value="KullanÄ±cÄ±nÄ±n Jotform hakkÄ±ndaki sorularÄ±nÄ± yanÄ±tlamak ve onlara platformu en verimli ÅŸekilde nasÄ±l kullanacaklarÄ± konusunda rehberlik etmek.")
        
        submit_button = st.form_submit_button(label="ğŸ§ª Senaryoyu DeÄŸerlendir", use_container_width=True, type="primary")

    if submit_button:
        if not all([user_query, agent_response, agent_persona, agent_goal]):
            st.warning("LÃ¼tfen deÄŸerlendirme yapmadan Ã¶nce tÃ¼m alanlarÄ± doldurun.")
        elif evaluator:
            with st.spinner("Senaryo deÄŸerlendiriliyor..."):
                rag_context = f"Agent'Ä±n bilgi tabanÄ±ndan getirdiÄŸi varsayÄ±lan kanÄ±t: '{agent_response}'"
                
                result = asyncio.run(evaluator.evaluate_conversation(
                    user_query=user_query,
                    agent_response=agent_response,
                    agent_goal=agent_goal,
                    rag_context=rag_context,
                    agent_persona=agent_persona,
                    tool_calls=None
                ))
                # Sonucu session_state'e kaydet ki geri bildirim iÃ§in kullanÄ±labilsin
                st.session_state.eval_result = result
        else:
            st.error("DeÄŸerlendirici servisi (Evaluator) baÅŸlatÄ±lamadÄ±.")

    # Sandbox formu gÃ¶nderildikten sonra sonucu gÃ¶ster
    if 'eval_result' in st.session_state:
        display_evaluation_results(st.session_state.eval_result)

# --- Sayfa 2: Toplu DeÄŸerlendirme ---
elif page == "Toplu DeÄŸerlendirme":
    st.header("ğŸ“š Dosya YÃ¼kleyerek Toplu DeÄŸerlendirme")
    st.markdown("""
    Sohbet geÃ§miÅŸini iÃ§eren bir `.csv` dosyasÄ± yÃ¼kleyerek tÃ¼m konuÅŸmalarÄ± **arka planda** otomatik olarak deÄŸerlendirin.
    Bu iÅŸlem sÄ±rasÄ±nda uygulamada gezinmeye devam edebilirsiniz.
    """)

    uploaded_file = st.file_uploader("Sohbet (.csv) dosyasÄ±nÄ± seÃ§in", type="csv")

    if uploaded_file is not None:
        try:
            uploaded_chats_df = pd.read_csv(uploaded_file)
            data_path = "src/data"
            personas_df = pd.read_csv(os.path.join(data_path, "ai_agent_persona_june_18_25.csv"))
            tasks_df = pd.read_csv(os.path.join(data_path, "ai_agent_tasks_june_18_25.csv"))
            batch_data = process_chat_data(uploaded_chats_df, personas_df, tasks_df)

            st.info(f"YÃ¼klenen dosyadan deÄŸerlendirilmeye hazÄ±r {len(batch_data)} konuÅŸma bulundu.")

            if st.button(f"ğŸ“š {len(batch_data)} KonuÅŸmayÄ± Arka Planda DeÄŸerlendir", key="eval_batch_async", use_container_width=True, type="primary"):
                if not batch_data.empty:
                    # DataFrame'i Celery'ye gÃ¶ndermek iÃ§in JSON'a Ã§evir
                    batch_data_json = batch_data.to_json(orient='split')
                    task = batch_evaluate_task.delay(batch_data_json)
                    st.session_state['batch_task_id'] = task.id
                    st.success(f"Toplu deÄŸerlendirme gÃ¶revi baÅŸlatÄ±ldÄ±! GÃ¶rev ID: {task.id}")
                    st.info("Ä°lerleme durumu aÅŸaÄŸÄ±da gÃ¶sterilecektir. Bu sÄ±rada baÅŸka sayfalara gidebilirsiniz.")
                else:
                    st.warning("Ä°ÅŸlenecek geÃ§erli bir konuÅŸma bulunamadÄ±.")
        except Exception as e:
            st.error(f"Dosya iÅŸlenirken veya gÃ¶rev baÅŸlatÄ±lÄ±rken bir hata oluÅŸtu: {e}")

    # --- GÃ¶rev Durumunu Kontrol Etme ve SonuÃ§larÄ± GÃ¶sterme ---
    if 'batch_task_id' in st.session_state:
        task_id = st.session_state['batch_task_id']
        task_result = AsyncResult(task_id, app=celery_app)

        st.markdown("---")
        st.subheader("GÃ¶rev Ä°lerleme Durumu")
        
        if task_result.ready():
            if task_result.successful():
                st.success(f"GÃ¶rev (ID: {task_id}) baÅŸarÄ±yla tamamlandÄ±!")
                results = task_result.get()
                results_df = pd.DataFrame(results)

                # SonuÃ§larÄ± metrik sÃ¼tunlarÄ±na ayÄ±r
                for metric in ["goal_adherence", "answer_relevance", "groundedness", "persona_compliance", "style_and_courtesy", "conciseness", "knowledge_boundary_violation", "security_policy_violation"]:
                    if metric in results_df.columns and not results_df.empty:
                        results_df[f'{metric}_score'] = results_df[metric].apply(lambda x: x['score'] if isinstance(x, dict) else None)
                        results_df[f'{metric}_reasoning'] = results_df[metric].apply(lambda x: x['reasoning'] if isinstance(x, dict) else None)
                
                # Genel istatistikleri gÃ¶ster
                st.subheader("Genel Metrikler")
                if not results_df.empty:
                    avg_cols = st.columns(4)
                    avg_cols[0].metric("Toplam DeÄŸerlendirme", len(results_df))
                    avg_cols[1].metric("Ort. Groundedness", f"{results_df['groundedness_score'].mean():.2f}")
                    avg_cols[2].metric("Ort. Relevance", f"{results_df['answer_relevance_score'].mean():.2f}")
                    avg_cols[3].metric("Ort. Style", f"{results_df['style_and_courtesy_score'].mean():.2f}")
                
                # DetaylÄ± sonuÃ§larÄ± gÃ¶ster
                with st.expander("TÃ¼m DeÄŸerlendirme SonuÃ§larÄ±nÄ± GÃ¶r"):
                    st.dataframe(results_df)

                # SonuÃ§larÄ± gÃ¶sterdikten sonra task id'yi temizle
                del st.session_state['batch_task_id']
            else:
                st.error(f"GÃ¶rev (ID: {task_id}) bir hatayla sonuÃ§landÄ±: {task_result.info}")
                del st.session_state['batch_task_id']
        else:
            # GÃ¶rev hala Ã§alÄ±ÅŸÄ±yor, ilerlemeyi gÃ¶ster
            progress_meta = task_result.info or {'current': 0, 'total': 1}
            current = progress_meta.get('current', 0)
            total = progress_meta.get('total', 1)
            
            progress_percent = (current / total) if total > 0 else 0
            st.progress(progress_percent, text=f"DeÄŸerlendiriliyor... ({current}/{total})")
            
            # SayfanÄ±n periyodik olarak yenilenmesini tetikle
            time.sleep(5)
            st.rerun()

# --- Sayfa 3: Oturum DeÄŸerlendirme ---
elif page == "Oturum Analizi":
    st.header("ğŸ’¬ Sohbet Oturumu SeÃ§imi ve Analizi")
    data_path = "src/data"
    session_raw_data = load_and_merge_raw_data(data_path)

    if not session_raw_data.empty:
        chat_ids = session_raw_data['chat_id'].unique()
        
        # KullanÄ±cÄ± yeni bir oturum seÃ§tiÄŸinde eski sonuÃ§larÄ± temizlemek iÃ§in bir callback
        def on_chat_id_change():
            if 'session_eval_result' in st.session_state:
                del st.session_state['session_eval_result']

        selected_chat_id = st.selectbox(
            "DeÄŸerlendirilecek Oturumu SeÃ§in (Chat ID):", 
            chat_ids,
            key="chat_id_selector",
            on_change=on_chat_id_change
        )

        if selected_chat_id:
            session_df = session_raw_data[session_raw_data['chat_id'] == selected_chat_id].copy()
            
            st.subheader(f"Oturum DÃ¶kÃ¼mÃ¼: `{selected_chat_id}`")
            with st.container(height=400):
                for _, row in session_df.sort_values('created_at').iterrows():
                    user_type = str(row.get('type', 'assistant')).upper()
                    with st.chat_message(name="user" if user_type == 'USER' else "assistant"):
                        st.markdown(str(row.get('content', '')))
            
            with st.expander("Agent GÃ¶rev ve Persona DetaylarÄ±"):
                if not session_df.empty and 'persona' in session_df.columns:
                    st.code(str(session_df['persona'].iloc[0]), language=None)

            if st.button("ğŸš€ Bu Oturumu DeÄŸerlendir", key="eval_session", use_container_width=True, type="primary"):
                if evaluator and not session_df.empty:
                    with st.spinner("Oturum deÄŸerlendiriliyor..."):
                        result = asyncio.run(run_session_evaluation(session_df, evaluator))
                        # DeÄŸerlendirme sonucunu session_state'e kaydet
                        st.session_state['session_eval_result'] = result
                elif session_df.empty:
                    st.warning("DeÄŸerlendirilecek oturum verisi bulunamadÄ±.")
                else:
                    st.error("DeÄŸerlendirici servisi (Evaluator) baÅŸlatÄ±lamadÄ±.")

            # EÄŸer session_state'de bir sonuÃ§ varsa, onu gÃ¶ster
            if 'session_eval_result' in st.session_state:
                display_evaluation_results(st.session_state['session_eval_result'])
                
    else:
        st.error("VarsayÄ±lan veri yÃ¼klenemedi.") 